{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient Similarity Network GNN Proof of Concept\n",
    "\n",
    "A. Lundervold w/ Claude 3.5 Sonnet , 2025-01-07\n",
    "\n",
    "[![Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MMIV-ML/BMED365-2025/blob/main/Lab2-DL/notebooks/05-psn-graph-neural-networks.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll help you explore Patient Similarity Networks (PSNs) and Graph Neural Networks (GNNs) in the context of healthcare analytics. Let me break this down systematically:\n",
    "\n",
    "1. Patient Similarity Networks:\n",
    "- PSNs are mathematical representations where each node represents a patient\n",
    "- Edges between nodes indicate similarity between patients based on various medical features\n",
    "- These features typically include:\n",
    "  - Demographics (age, gender, ethnicity)\n",
    "  - Clinical measurements (lab results, vital signs)\n",
    "  - Diagnoses and procedures (ICD codes)\n",
    "  - Medications\n",
    "  - Genetic information when available\n",
    "\n",
    "2. Creating Patient Similarity:\n",
    "- Distance/similarity metrics are used to quantify patient relationships\n",
    "- Common approaches include:\n",
    "  - Euclidean distance for continuous variables\n",
    "  - Jaccard similarity for categorical variables\n",
    "  - Custom hybrid metrics for mixed data types\n",
    "- Feature weighting is often applied to prioritize more relevant characteristics\n",
    "- Dimensionality reduction techniques may be used to handle high-dimensional patient data\n",
    "\n",
    "3. GNNs for Patient Networks:\n",
    "- GNNs are particularly well-suited for analyzing PSNs because they can:\n",
    "  - Learn from both node features and network structure\n",
    "  - Aggregate information from similar patients\n",
    "  - Generate embeddings that capture complex patient relationships\n",
    "- Common GNN architectures for healthcare:\n",
    "  - GraphSAGE for handling large-scale patient networks\n",
    "  - GAT (Graph Attention Networks) for learning which patient similarities are most important\n",
    "  - GCN (Graph Convolutional Networks) for basic feature aggregation\n",
    "\n",
    "4. Applications:\n",
    "- Disease prediction and risk stratification\n",
    "- Treatment recommendation based on similar patients' outcomes\n",
    "- Patient subgroup discovery\n",
    "- Personalized medicine approaches\n",
    "- Clinical trial matching\n",
    "\n",
    "5. Technical Implementation Considerations:\n",
    "- Handling missing data\n",
    "- Dealing with temporal aspects of patient data\n",
    "- Ensuring privacy and security\n",
    "- Scalability for large patient populations\n",
    "- Interpretability requirements for clinical use\n",
    "\n",
    "Would you like me to dive deeper into any of these aspects? I can also provide code examples for implementing PSNs or GNNs for specific healthcare applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Can you make a prototype PoC in python?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll create a proof of concept that demonstrates building a Patient Similarity Network and applying a simple GNN for patient outcome prediction. I'll use PyTorch Geometric for the GNN implementation.\n",
    "\n",
    "\n",
    "\n",
    "This PoC demonstrates several key concepts:\n",
    "\n",
    "1. Patient Similarity Network Creation:\n",
    "- Features are preprocessed and standardized\n",
    "- Cosine similarity is used to measure patient similarity\n",
    "- A threshold is applied to create the adjacency matrix\n",
    "\n",
    "2. GNN Architecture:\n",
    "- Uses 3 Graph Convolutional layers\n",
    "- Includes dropout for regularization\n",
    "- Binary classification output (could predict patient outcomes, readmission risk, etc.)\n",
    "\n",
    "3. Training Pipeline:\n",
    "- Synthetic data generation for demonstration\n",
    "- Model training with binary cross-entropy loss\n",
    "- Basic evaluation metrics\n",
    "\n",
    "To extend this PoC for real-world use, you would want to:\n",
    "\n",
    "1. Enhance feature processing:\n",
    "- Add proper categorical variable encoding\n",
    "- Implement more sophisticated missing value handling\n",
    "- Add feature selection/importance analysis\n",
    "\n",
    "2. Improve the network structure:\n",
    "- Implement multiple similarity metrics\n",
    "- Add edge weights based on similarity scores\n",
    "- Consider temporal aspects of patient data\n",
    "\n",
    "3. Expand the model:\n",
    "- Add attention mechanisms\n",
    "- Implement more sophisticated architectures (GraphSAGE, GAT)\n",
    "- Add interpretability components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll enhance the model with a Graph Attention Network (GAT) layer and add interpretability components to help understand the model's decisions.\n",
    "\n",
    "I'll enhance the model with a Graph Attention Network (GAT) layer and add interpretability components to help understand the model's decisions.\n",
    "\n",
    "\n",
    "\n",
    "Key enhancements in this version:\n",
    "\n",
    "1. Attention Mechanisms:\n",
    "- Replaced GCN with GATv2Conv (improved version of GAT)\n",
    "- Added multi-head attention (4 heads per layer)\n",
    "- Stores attention weights for interpretation\n",
    "- Feature-level attention through the feature_attention layer\n",
    "\n",
    "2. Interpretability Components:\n",
    "- ModelInterpreter class for explaining predictions\n",
    "- Feature importance visualization\n",
    "- Neighbor attention weight visualization\n",
    "- Training monitoring with loss and accuracy tracking\n",
    "\n",
    "3. Enhanced Network Structure:\n",
    "- Weighted adjacency matrix based on similarity scores\n",
    "- Better feature preprocessing\n",
    "- Meaningful feature names for interpretability\n",
    "\n",
    "4. Model Architecture:\n",
    "- Three GAT layers with skip connections\n",
    "- Dropout and ELU activation functions\n",
    "- Feature importance layer\n",
    "\n",
    "5. Visualization:\n",
    "- Feature importance bar plots\n",
    "- Attention weight heatmaps\n",
    "- Training progress monitoring\n",
    "\n",
    "To use this enhanced version:\n",
    "\n",
    "```python\n",
    "# Train model and get interpreter\n",
    "model, interpreter, training_stats = main()\n",
    "\n",
    "# Explain prediction for a specific patient\n",
    "patient_idx = 0\n",
    "explanation = interpreter.explain_prediction(patient_idx)\n",
    "\n",
    "# Visualize explanation\n",
    "interpreter.visualize_explanation(patient_idx)\n",
    "```\n",
    "\n",
    "Would you like me to:\n",
    "1. Add more sophisticated visualization options?\n",
    "2. Implement additional interpretability metrics?\n",
    "3. Add support for temporal patient data?\n",
    "4. Enhance the attention mechanism further?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll enhance the code with temporal support, advanced visualizations, and additional interpretability metrics.\n",
    "\n",
    "\n",
    "\n",
    "Key enhancements in this version:\n",
    "\n",
    "1. Temporal Data Support:\n",
    "- Added `TemporalPatientFeatures` class for processing time series data\n",
    "- Implemented Dynamic Time Warping for temporal similarity\n",
    "- Added temporal attention mechanism\n",
    "- Created synthetic temporal data generator with trends and seasonality\n",
    "\n",
    "2. Advanced Visualizations:\n",
    "- Time series patterns visualization\n",
    "- Network structure visualization using NetworkX\n",
    "- UMAP embeddings for patient clustering\n",
    "- Community structure analysis\n",
    "- Feature correlation heatmaps\n",
    "- Degree distribution plots\n",
    "\n",
    "3. Enhanced Interpretability:\n",
    "- SHAP values for feature importance\n",
    "- Feature interaction analysis\n",
    "- Temporal pattern analysis\n",
    "- Community detection\n",
    "- Network topology metrics\n",
    "- Correlation analysis\n",
    "\n",
    "4. Additional Metrics:\n",
    "- Node centrality measures\n",
    "- Community detection\n",
    "- Feature interaction strengths\n",
    "- Temporal pattern similarity\n",
    "\n",
    "To use the enhanced features:\n",
    "\n",
    "```python\n",
    "# Train model and get interpreter\n",
    "model, interpreter, temporal_df = main()\n",
    "\n",
    "# Generate visualizations for a patient\n",
    "patient_idx = 0\n",
    "temporal_viz = interpreter.visualize_temporal_patterns(\n",
    "    patient_idx,\n",
    "    temporal_df[temporal_df['patient_id'] == patient_idx].set_index('timestamp')\n",
    ")\n",
    "\n",
    "# Visualize network structure\n",
    "network_viz = interpreter.visualize_network_structure()\n",
    "\n",
    "# Get SHAP values for interpretation\n",
    "shap_values = interpreter.compute_shap_values(patient_idx)\n",
    "\n",
    "# Analyze feature interactions\n",
    "interactions = interpreter.compute_feature_interactions(patient_idx)\n",
    "```\n",
    "\n",
    "Would you like me to:\n",
    "1. Add more advanced temporal patterns analysis (e.g., change point detection, trend analysis)?\n",
    "2. Implement additional network metrics or community detection algorithms?\n",
    "3. Add support for handling missing temporal data?\n",
    "4. Enhance the visualization options further with interactive plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch_geometric\n",
    "!pip install umap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, GlobalAttention\n",
    "from torch_geometric.data import Data, Batch\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import networkx as nx\n",
    "from scipy.stats import spearmanr\n",
    "import umap\n",
    "import shap\n",
    "\n",
    "class TemporalPatientFeatures:\n",
    "    def __init__(self, max_sequence_length=10):\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.numerical_scaler = StandardScaler()\n",
    "        self.temporal_scaler = MinMaxScaler()\n",
    "        \n",
    "    def process_temporal_data(self, temporal_df):\n",
    "        \"\"\"\n",
    "        Process temporal patient data with variable sequence lengths\n",
    "        \"\"\"\n",
    "        # Drop non-feature columns for sequence creation\n",
    "        feature_cols = [col for col in temporal_df.columns \n",
    "                       if col not in ['patient_id', 'timestamp']]\n",
    "        \n",
    "        # Group by patient and sort by timestamp\n",
    "        grouped = temporal_df.groupby('patient_id', as_index=False)\n",
    "        \n",
    "        # Pad or truncate sequences\n",
    "        padded_sequences = []\n",
    "        for _, patient_data in grouped:\n",
    "            # Sort by timestamp and select features\n",
    "            sequence = (patient_data.sort_values('timestamp')\n",
    "                      [feature_cols]\n",
    "                      .values[:self.max_sequence_length])\n",
    "            \n",
    "            # Pad if necessary\n",
    "            if len(sequence) < self.max_sequence_length:\n",
    "                padding = np.zeros((self.max_sequence_length - len(sequence), \n",
    "                                  len(feature_cols)))\n",
    "                sequence = np.vstack([sequence, padding])\n",
    "            \n",
    "            padded_sequences.append(sequence)\n",
    "            \n",
    "        return np.array(padded_sequences)\n",
    "\n",
    "class TemporalPatientSimilarityNetwork:\n",
    "    def __init__(self, similarity_threshold=0.7):\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.feature_processor = TemporalPatientFeatures()\n",
    "        \n",
    "    def compute_temporal_similarity(self, seq1, seq2):\n",
    "        \"\"\"\n",
    "        Compute similarity between two temporal sequences\n",
    "        \"\"\"\n",
    "        # Dynamic Time Warping distance\n",
    "        dtw_distance = self._dtw_distance(seq1, seq2)\n",
    "        # Normalize to similarity score\n",
    "        similarity = 1 / (1 + dtw_distance)\n",
    "        return similarity\n",
    "    \n",
    "    def _dtw_distance(self, seq1, seq2):\n",
    "        \"\"\"\n",
    "        Compute Dynamic Time Warping distance between sequences\n",
    "        \"\"\"\n",
    "        n, m = len(seq1), len(seq2)\n",
    "        dtw_matrix = np.inf * np.ones((n + 1, m + 1))\n",
    "        dtw_matrix[0, 0] = 0\n",
    "        \n",
    "        for i in range(1, n + 1):\n",
    "            for j in range(1, m + 1):\n",
    "                cost = np.linalg.norm(seq1[i-1] - seq2[j-1])\n",
    "                dtw_matrix[i, j] = cost + min(dtw_matrix[i-1, j],\n",
    "                                            dtw_matrix[i, j-1],\n",
    "                                            dtw_matrix[i-1, j-1])\n",
    "        \n",
    "        return dtw_matrix[n, m]\n",
    "\n",
    "class TemporalGAT(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=32, num_heads=4):\n",
    "        super(TemporalGAT, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        # Temporal attention layer\n",
    "        self.temporal_attention = GlobalAttention(\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(hidden_dim, 1)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # GAT layers\n",
    "        self.gat_layers = torch.nn.ModuleList([\n",
    "            GATv2Conv(num_features, hidden_dim, heads=num_heads, dropout=0.2),\n",
    "            GATv2Conv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=0.2),\n",
    "            GATv2Conv(hidden_dim * num_heads, 1, heads=1, concat=False)\n",
    "        ])\n",
    "        \n",
    "        # Feature importance layers\n",
    "        self.feature_attention = torch.nn.Linear(num_features, num_features)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        # Store attention weights\n",
    "        self.attention_weights = []\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_weights = torch.sigmoid(self.feature_attention(x))\n",
    "        x = x * feature_weights\n",
    "        \n",
    "        # Apply GAT layers\n",
    "        for layer in self.gat_layers[:-1]:\n",
    "            x, attention = layer(x, edge_index, return_attention_weights=True)\n",
    "            self.attention_weights.append(attention)\n",
    "            x = F.elu(x)\n",
    "            x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Final layer\n",
    "        x, attention = self.gat_layers[-1](x, edge_index, return_attention_weights=True)\n",
    "        self.attention_weights.append(attention)\n",
    "        \n",
    "        return torch.sigmoid(x), feature_weights\n",
    "\n",
    "class AdvancedModelInterpreter:\n",
    "    def __init__(self, model, data, feature_names):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.feature_names = feature_names\n",
    "        self.explainer = shap.DeepExplainer(model, data)\n",
    "        \n",
    "    def compute_shap_values(self, patient_idx):\n",
    "        \"\"\"\n",
    "        Compute SHAP values for feature importance\n",
    "        \"\"\"\n",
    "        shap_values = self.explainer.shap_values(self.data.x[patient_idx:patient_idx+1])\n",
    "        return shap_values[0]\n",
    "    \n",
    "    def compute_feature_interactions(self, patient_idx):\n",
    "        \"\"\"\n",
    "        Compute feature interaction strengths\n",
    "        \"\"\"\n",
    "        features = self.data.x[patient_idx].detach().numpy()\n",
    "        interactions = np.zeros((len(features), len(features)))\n",
    "        \n",
    "        for i in range(len(features)):\n",
    "            for j in range(i+1, len(features)):\n",
    "                corr, _ = spearmanr(features[i], features[j])\n",
    "                interactions[i, j] = abs(corr)\n",
    "                interactions[j, i] = abs(corr)\n",
    "                \n",
    "        return interactions\n",
    "    \n",
    "    def visualize_temporal_patterns(self, patient_idx, temporal_data):\n",
    "        \"\"\"\n",
    "        Visualize temporal patterns for a patient\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Time series plot\n",
    "        temporal_data.plot(ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Temporal Feature Patterns')\n",
    "        \n",
    "        # Feature correlation heatmap\n",
    "        sns.heatmap(temporal_data.corr(), ax=axes[0, 1], cmap='coolwarm')\n",
    "        axes[0, 1].set_title('Feature Correlations Over Time')\n",
    "        \n",
    "        # SHAP values\n",
    "        shap_values = self.compute_shap_values(patient_idx)\n",
    "        shap.summary_plot(shap_values, self.data.x[patient_idx:patient_idx+1],\n",
    "                         feature_names=self.feature_names, ax=axes[1, 0], show=False)\n",
    "        axes[1, 0].set_title('SHAP Feature Importance')\n",
    "        \n",
    "        # Feature interactions\n",
    "        interactions = self.compute_feature_interactions(patient_idx)\n",
    "        sns.heatmap(interactions, ax=axes[1, 1],\n",
    "                   xticklabels=self.feature_names,\n",
    "                   yticklabels=self.feature_names,\n",
    "                   cmap='YlOrRd')\n",
    "        axes[1, 1].set_title('Feature Interactions')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def visualize_network_structure(self):\n",
    "        \"\"\"\n",
    "        Visualize the patient similarity network structure\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Convert to networkx graph\n",
    "        edge_index = self.data.edge_index.numpy()\n",
    "        G = nx.Graph()\n",
    "        for i in range(edge_index.shape[1]):\n",
    "            G.add_edge(edge_index[0, i], edge_index[1, i])\n",
    "        \n",
    "        # Node embedding visualization using UMAP\n",
    "        reducer = umap.UMAP(random_state=42)\n",
    "        embedding = reducer.fit_transform(self.data.x.detach().numpy())\n",
    "        \n",
    "        # Network structure\n",
    "        pos = nx.spring_layout(G)\n",
    "        nx.draw(G, pos, ax=axes[0, 0], node_size=100, node_color='lightblue')\n",
    "        axes[0, 0].set_title('Patient Similarity Network')\n",
    "        \n",
    "        # UMAP embedding\n",
    "        axes[0, 1].scatter(embedding[:, 0], embedding[:, 1], c='blue', alpha=0.5)\n",
    "        axes[0, 1].set_title('UMAP Patient Embedding')\n",
    "        \n",
    "        # Degree distribution\n",
    "        degrees = [d for n, d in G.degree()]\n",
    "        axes[1, 0].hist(degrees, bins=20)\n",
    "        axes[1, 0].set_title('Node Degree Distribution')\n",
    "        \n",
    "        # Community structure\n",
    "        communities = nx.community.greedy_modularity_communities(G)\n",
    "        comm_sizes = [len(c) for c in communities]\n",
    "        axes[1, 1].bar(range(len(comm_sizes)), sorted(comm_sizes, reverse=True))\n",
    "        axes[1, 1].set_title('Community Size Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "def create_synthetic_temporal_data(num_patients=100, num_features=10, \n",
    "                                 sequence_length=10):\n",
    "    \"\"\"\n",
    "    Create synthetic temporal patient data with realistic patterns\n",
    "    \"\"\"\n",
    "    # Create feature names\n",
    "    feature_names = [\n",
    "        'age', 'bmi', 'blood_pressure', 'glucose',\n",
    "        'heart_rate', 'cholesterol', 'hemoglobin',\n",
    "        'white_blood_cells', 'platelets', 'creatinine'\n",
    "    ][:num_features]\n",
    "    \n",
    "    # Generate base date range\n",
    "    base_date = datetime(2024, 1, 1)\n",
    "    date_range = [base_date + timedelta(days=i) for i in range(sequence_length)]\n",
    "    \n",
    "    # Create empty list to store all records\n",
    "    all_records = []\n",
    "    \n",
    "    for patient_id in range(num_patients):\n",
    "        # Generate patient-specific baseline values\n",
    "        baseline = np.random.randn(num_features)\n",
    "        \n",
    "        # Generate time series with trend and seasonality\n",
    "        time = np.linspace(0, 2*np.pi, sequence_length)\n",
    "        trend = np.linspace(0, 0.5, sequence_length)  # Reduced trend magnitude\n",
    "        seasonal = 0.2 * np.sin(time)  # Reduced seasonality magnitude\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "            # Generate features with baseline, trend, and seasonality\n",
    "            features = baseline + trend[t] + seasonal[t] + 0.1 * np.random.randn(num_features)\n",
    "            \n",
    "            # Create record\n",
    "            record = {\n",
    "                'patient_id': patient_id,\n",
    "                'timestamp': date_range[t]\n",
    "            }\n",
    "            \n",
    "            # Add features\n",
    "            for feat_idx, feat_name in enumerate(feature_names):\n",
    "                record[feat_name] = features[feat_idx]\n",
    "            \n",
    "            all_records.append(record)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_records)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # Generate synthetic temporal data\n",
    "    temporal_df = create_synthetic_temporal_data()\n",
    "    \n",
    "    # Process temporal data\n",
    "    feature_processor = TemporalPatientFeatures()\n",
    "    temporal_sequences = feature_processor.process_temporal_data(temporal_df)\n",
    "    \n",
    "    # Build similarity network\n",
    "    network = TemporalPatientSimilarityNetwork()\n",
    "    similarity_matrix = np.zeros((len(temporal_sequences), len(temporal_sequences)))\n",
    "    \n",
    "    for i in range(len(temporal_sequences)):\n",
    "        for j in range(i+1, len(temporal_sequences)):\n",
    "            sim = network.compute_temporal_similarity(\n",
    "                temporal_sequences[i],\n",
    "                temporal_sequences[j]\n",
    "            )\n",
    "            similarity_matrix[i, j] = sim\n",
    "            similarity_matrix[j, i] = sim\n",
    "    \n",
    "    # Create PyTorch Geometric data\n",
    "    edge_index = torch.tensor(np.where(similarity_matrix > network.similarity_threshold),\n",
    "                            dtype=torch.long)\n",
    "    x = torch.tensor(temporal_sequences[:, -1, :], dtype=torch.float)\n",
    "    y = torch.tensor(np.random.binomial(1, 0.3, len(temporal_sequences)),\n",
    "                    dtype=torch.float)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, y=y)\n",
    "    \n",
    "    # Initialize and train model\n",
    "    model = TemporalGAT(num_features=temporal_sequences.shape[2])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Initialize interpreter\n",
    "    interpreter = AdvancedModelInterpreter(\n",
    "        model,\n",
    "        data,\n",
    "        feature_names=temporal_df.columns[2:].tolist()\n",
    "    )\n",
    "    \n",
    "    # Generate visualizations\n",
    "    sample_patient_idx = 0\n",
    "    temporal_viz = interpreter.visualize_temporal_patterns(\n",
    "        sample_patient_idx,\n",
    "        temporal_df[temporal_df['patient_id'] == sample_patient_idx].set_index('timestamp')\n",
    "    )\n",
    "    network_viz = interpreter.visualize_network_structure()\n",
    "    \n",
    "    return model, interpreter, temporal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, interpreter, temporal_df = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
