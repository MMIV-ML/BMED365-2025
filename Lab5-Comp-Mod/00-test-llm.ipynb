{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and explore `llm` with DeepSeek-R1 local model\n",
    "\n",
    "Arvid Lundervold, 2025-01-25\n",
    "\n",
    "(Using the `bmed365-cuda` environment)\n",
    "\n",
    "\n",
    "**DeepSeek-R1**:\n",
    "- https://github.com/deepseek-ai/DeepSeek-R1\n",
    "- https://github.com/deepseek-ai/awesome-deepseek-integration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simon Willison's LLM tool\n",
    "Simon Willison's LLM tool (https://github.com/simonw/llm) can be installed using several methods:\n",
    "\n",
    "### Using pip\n",
    "\n",
    "The simplest way to install LLM is using pip (in the bmed365-cuda environment):\n",
    "\n",
    "```bash\n",
    "pip install llm\n",
    "```\n",
    "\n",
    "### Using pipx (recommended)\n",
    "\n",
    "For a more isolated installation, you can use pipx:\n",
    "\n",
    "```bash\n",
    "pipx install llm\n",
    "```\n",
    "\n",
    "#### Using Homebrew\n",
    "\n",
    "If you're on macOS or Linux and have Homebrew installed:\n",
    "\n",
    "```bash\n",
    "brew install llm\n",
    "```\n",
    "\n",
    "#### Using uv\n",
    "\n",
    "If you have the uv package manager installed:\n",
    "\n",
    "```bash\n",
    "uv tool install llm\n",
    "```\n",
    "\n",
    "After installation, you can verify that LLM is working by running:\n",
    "\n",
    "```bash\n",
    "llm -v\n",
    "```\n",
    "\n",
    "#### Setting up OpenAI API (optional)\n",
    "\n",
    "If you want to use OpenAI's models, you'll need to set up your API key:\n",
    "\n",
    "```bash\n",
    "llm keys set openai\n",
    "# Paste your OpenAI API key when prompted\n",
    "```\n",
    "\n",
    "#### Installing plugins\n",
    "\n",
    "LLM supports various plugins for additional functionality. For example, to install Ollama plugin for local models:\n",
    "\n",
    "```bash\n",
    "llm install llm-ollama\n",
    "```\n",
    "\n",
    "Remember to keep LLM updated. You can upgrade it using the same method you used to install it, such as `pip install -U llm` or `brew upgrade llm`[1][2][4].\n",
    "\n",
    "Citations:\n",
    "[1] https://llm.datasette.io/en/stable/setup.html<br>       \n",
    "[2] https://christianjmills.com/posts/mastering-llms-course-notes/conference-talk-017/<br>\n",
    "[3] https://github.com/simonw/llm-mlc<br>\n",
    "[4] https://parlance-labs.com/education/applications/simon_llm_cli/<br>\n",
    "[5] https://simonwillison.net/series/using-llms/<br>\n",
    "[6] https://github.com/simonw/llm-mlc<br>\n",
    "[7] https://www.youtube.com/watch?v=ngWJbdw1NrQ<br>\n",
    "[8] https://nelsonslog.wordpress.com/2023/08/16/running-my-own-llm/<br> \n",
    "[9] https://github.com/simonw/llm/tree/refs/tags/0.20<br>\n",
    "[10] https://llm.datasette.io/en/stable/<br>\n",
    "[11] https://simonwillison.net/2023/Jul/12/llm/<br>\n",
    "[12] https://parlance-labs.com/education/applications/simon_llm_cli/<br>\n",
    "[13] https://www.youtube.com/watch?v=QUXQNi6jQ30<br>\n",
    "[14] https://github.com/simonw/llm-gemini<br>\n",
    "[15] https://pypi.org/project/datasette-llm-embed/<br>\n",
    "[16] https://simonwillison.net/2024/Jun/17/cli-language-models/<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSeek-R1\n",
    "\n",
    "To install DeepSeek-R1 in Simon Willison's LLM tool, you can follow these steps:\n",
    "\n",
    "1. First, ensure you have LLM installed (if not, install it using pip)\n",
    "\n",
    "```bash\n",
    "llm --version\n",
    "```\n",
    "\n",
    "2. Install the llm-ollama plugin:\n",
    "\n",
    "```bash\n",
    "llm install llm-ollama\n",
    "```\n",
    "\n",
    "3. See **Ollama** below for how to install Ollama. <br>Use **Ollama to download and run the DeepSeek-R1 model**. We will need to install a **Destilled Model**, as the full model is too large to run locally. <br> Check out https://ollama.com/library/deepseek-r1 and the [DeepSeek-R1](https://simonwillison.net/2025/Jan/20/deepseek-r1/) page for more information.\n",
    "\n",
    "Open your terminal and execute:\n",
    "\n",
    "```bash\n",
    "ollama run deepseek-r1:70b   OR (https://ollama.com/library/deepseek-r1)\n",
    "ollama run deepseek-r1:8b\n",
    "```\n",
    "\n",
    "This will download the model, which may take some time depending on your internet speed.\n",
    "\n",
    "4. Once the model is downloaded, you can use it with LLM. Set up an alias for easier access:\n",
    "\n",
    "```bash\n",
    "llm aliases set r1 deepseek-r1:70b\n",
    "```\n",
    "\n",
    "5. Now you can use DeepSeek-R1 in LLM with the following command:\n",
    "\n",
    "```bash\n",
    "llm -m r1 \"Your prompt here\"\n",
    "```\n",
    "\n",
    "For a specific example using a distilled version of DeepSeek-R1, you can use:\n",
    "\n",
    "```bash\n",
    "llm -m 'hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:Q8_0' 'Your prompt here'\n",
    "```\n",
    "\n",
    "This method allows you to use DeepSeek-R1 locally through LLM, leveraging Ollama for model management[1][4][7].\n",
    "\n",
    "Citations:\n",
    "\n",
    "[1] https://simonwillison.net/2025/Jan/20/deepseek-r1/<br>  \n",
    "[2] https://apidog.com/blog/run-deepseek-r1-locally-with-ollama/<br>\n",
    "[3] https://www.youtube.com/watch?v=DCqqCLlsIBU<br>\n",
    "[4] https://simonw.substack.com/p/the-deepseek-r1-family-of-reasoning<br>\n",
    "[5] https://aleksandarhaber.com/install-and-run-locally-deepseek-r1-ai-model-on-windows/<br>\n",
    "[6] https://www.youtube.com/watch?v=hruG-JlLeQg<br>\n",
    "[7] https://news.ycombinator.com/item?id=42769222<br>\n",
    "[8] https://www.reddit.com/r/selfhosted/comments/1i6ggyh/got_deepseek_r1_running_locally_full_setup_guide/<br>  \n",
    "[9] https://dev.to/dwtoledo/how-to-use-deepseek-r1-for-free-in-visual-studio-code-with-cline-or-roo-code-3an9<br>\n",
    "[10] https://dev.to/dwtoledo/how-to-use-deepseek-r1-for-free-in-visual-studio-code-with-cline-or-roo-code-3an9<br>\n",
    "[11] https://chat.deepseek.com<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "Ollama is a powerful tool for running large language models locally. Here's how to install it on different platforms:\n",
    "\n",
    "### Linux Installation\n",
    "\n",
    "To install Ollama on Linux, you can use the following command in your terminal[1][4]:\n",
    "\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "This will download and run the installation script. After installation, you can verify that Ollama is running by executing:\n",
    "\n",
    "```bash\n",
    "ollama -v\n",
    "```\n",
    "\n",
    "#### Manual Installation\n",
    "\n",
    "For a manual installation on Linux[1]:\n",
    "\n",
    "1. Download the package:\n",
    "```bash\n",
    "curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\n",
    "```\n",
    "\n",
    "2. Extract the package:\n",
    "```bash\n",
    "sudo tar -C /usr -xzf ollama-linux-amd64.tgz\n",
    "```\n",
    "\n",
    "3. Start Ollama:\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "### macOS Installation\n",
    "\n",
    "For macOS users, the installation process is straightforward[6]:\n",
    "\n",
    "1. Visit ollama.com and click on the download button.\n",
    "2. Open the downloaded file.\n",
    "3. Move Ollama to the Applications folder when prompted.\n",
    "4. Follow the on-screen instructions to install the command-line tool.\n",
    "\n",
    "After installation, you can run Ollama from the terminal using commands like `ollama run llama2`.\n",
    "\n",
    "### Windows Installation\n",
    "\n",
    "To install Ollama on Windows[6]:\n",
    "\n",
    "1. Go to ollama.com and download the Windows installer (OllamaSetup.exe).\n",
    "2. Run the installer and follow the prompts.\n",
    "3. Once installed, you can use Ollama from PowerShell or Command Prompt.\n",
    "\n",
    "### Additional Setup\n",
    "\n",
    "After installation, you can start using Ollama immediately. To run a model, use the command:\n",
    "\n",
    "```bash\n",
    "ollama run llama2\n",
    "```\n",
    "\n",
    "This will download the specified model (in this case, llama2) and start an interactive session[6].\n",
    "\n",
    "### Optional: Installing OpenWebUI\n",
    "\n",
    "For those who prefer a graphical interface, you can install OpenWebUI, which provides a web-based frontend for Ollama[3]. The easiest method is to use Docker:\n",
    "\n",
    "1. Ensure Docker is installed on your system.\n",
    "2. Follow the installation instructions on the official OpenWebUI website (https://docs.openwebui.com/getting-started/).\n",
    "\n",
    "Remember to keep Ollama and your system updated for the best performance and security[2].\n",
    "\n",
    "Citations:\n",
    "[1] https://github.com/ollama/ollama/blob/main/docs/linux.md<br>\n",
    "[2] https://www.hostinger.com/tutorials/how-to-install-ollama<br>\n",
    "[3] https://www.doprax.com/tutorial/a-step-by-step-guide-for-installing-and-running-ollama-and-openwebui-locally-part-1/<br>\n",
    "[4] https://ollama.com/download<br>\n",
    "[5] https://snapcraft.io/install/ollama/ubuntu<br>\n",
    "[6] https://www.youtube.com/watch?v=oI7VoTM9NKQ<br>\n",
    "[7] https://www.youtube.com/watch?v=e3j1a2PKw1k<br>\n",
    "[8] https://itsfoss.com/ollama-setup-linux/<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arvid/anaconda3/envs/bmed365-cuda/bin/llm\n"
     ]
    }
   ],
   "source": [
    "!which llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm, version 0.20\n"
     ]
    }
   ],
   "source": [
    "!llm --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Chat: gpt-4o (aliases: 4o)\n",
      "OpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\n",
      "OpenAI Chat: gpt-4o-audio-preview\n",
      "OpenAI Chat: gpt-4o-audio-preview-2024-12-17\n",
      "OpenAI Chat: gpt-4o-audio-preview-2024-10-01\n",
      "OpenAI Chat: gpt-4o-mini-audio-preview\n",
      "OpenAI Chat: gpt-4o-mini-audio-preview-2024-12-17\n",
      "OpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\n",
      "OpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\n",
      "OpenAI Chat: gpt-4 (aliases: 4, gpt4)\n",
      "OpenAI Chat: gpt-4-32k (aliases: 4-32k)\n",
      "OpenAI Chat: gpt-4-1106-preview\n",
      "OpenAI Chat: gpt-4-0125-preview\n",
      "OpenAI Chat: gpt-4-turbo-2024-04-09\n",
      "OpenAI Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\n",
      "OpenAI Chat: o1\n",
      "OpenAI Chat: o1-2024-12-17\n",
      "OpenAI Chat: o1-preview\n",
      "OpenAI Chat: o1-mini\n",
      "OpenAI Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\n",
      "Ollama: deepseek-r1:70b\n",
      "Default: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "!llm models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_llm(prompt):\n",
    "    #binary_path = \"/usr/local/bin/llm\"\n",
    "    binary_path = \"/home/arvid/anaconda3/envs/bmed365-cuda/bin/llm\"\n",
    "    command = [binary_path, prompt]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, text=True, capture_output=True)\n",
    "        \n",
    "        print(\"LLM Response:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        if result.stderr:\n",
    "            print(\"Standard Error:\")\n",
    "            print(result.stderr)\n",
    "        \n",
    "        # print(f\"Return Code: {result.returncode}\")\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        print(f\"Standard Output: {e.stdout}\")\n",
    "        print(f\"Standard Error: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      "OpenAI Chat: gpt-4o (aliases: 4o)\n",
      "OpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\n",
      "OpenAI Chat: gpt-4o-audio-preview\n",
      "OpenAI Chat: gpt-4o-audio-preview-2024-12-17\n",
      "OpenAI Chat: gpt-4o-audio-preview-2024-10-01\n",
      "OpenAI Chat: gpt-4o-mini-audio-preview\n",
      "OpenAI Chat: gpt-4o-mini-audio-preview-2024-12-17\n",
      "OpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\n",
      "OpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\n",
      "OpenAI Chat: gpt-4 (aliases: 4, gpt4)\n",
      "OpenAI Chat: gpt-4-32k (aliases: 4-32k)\n",
      "OpenAI Chat: gpt-4-1106-preview\n",
      "OpenAI Chat: gpt-4-0125-preview\n",
      "OpenAI Chat: gpt-4-turbo-2024-04-09\n",
      "OpenAI Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\n",
      "OpenAI Chat: o1\n",
      "OpenAI Chat: o1-2024-12-17\n",
      "OpenAI Chat: o1-preview\n",
      "OpenAI Chat: o1-mini\n",
      "OpenAI Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\n",
      "Ollama: deepseek-r1:70b\n",
      "Default: gpt-4o-mini\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"models\"\n",
    "run_llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the DeepSeek-R1 model:\n",
    "\n",
    "Run in a consol window:\n",
    "```bash\n",
    "ollama run deepseek-r1:70b\n",
    "```\n",
    "\n",
    "Run in a notebook cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ollama run deepseek-r1:70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#prompt = \"A function that computes the lowest common multiples of two integers, and a function that runs 5 test cases of the lowest common multiple function\"\n",
    "#run_llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: https://github.com/arvidl/sagemath-examples/blob/master/07-ai-in-sage.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hva er diabetes?', 'Hvilke matvarer bør diabetikere unngå?', 'Hvordan fungerer insulinpumper?', 'Hvilke øvelser er trygge for diabetikere?', 'Hvordan takle depresjon som følge av diabetes?']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Temabasert ordliste\n",
    "temaer = {\n",
    "    \"sykdomsmekanismer\": [\"Hva er diabetes?\", \"Hvordan påvirker diabetes insulinproduksjonen?\"],\n",
    "    \"kostholdsråd\": [\"Hvilke matvarer bør diabetikere unngå?\", \"Hvordan balansere karbohydrater?\"],\n",
    "    \"medisinering\": [\"Hvilke medisiner brukes vanligvis til å behandle type 2 diabetes?\", \"Hvordan fungerer insulinpumper?\"],\n",
    "    \"livsstil\": [\"Hvilke øvelser er trygge for diabetikere?\", \"Hvordan kan stress påvirke blodsukkernivået?\"],\n",
    "    \"psykologisk støtte\": [\"Hvordan kan jeg snakke med familien min om min diabetes?\", \"Hvordan takle depresjon som følge av diabetes?\"]\n",
    "}\n",
    "\n",
    "# Spørsmålsgenerator\n",
    "def generer_spørsmål(tema):\n",
    "    spørsmål = random.choice(temaer[tema])\n",
    "    return spørsmål\n",
    "\n",
    "# Generere et sett med spørsmål\n",
    "def generer_spørsmålsett():\n",
    "    spørsmålsett = []\n",
    "    for tema in temaer.keys():\n",
    "        spørsmålsett.append(generer_spørsmål(tema))\n",
    "    return spørsmålsett\n",
    "\n",
    "# Eksempel på genererte spørsmål\n",
    "print(generer_spørsmålsett())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_liste = generer_spørsmålsett()\n",
    "len(spm_liste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "%%time\n",
    "for i, spm in enumerate(spm_liste):\n",
    "    print(f'Spm {i}: {spm}')\n",
    "    prompt = f\"{spm}. Svar på norsk. Bruk max 150 ord. Start med 'Svar på Spm {i}'. Ikke bruk avsnitt. -m gpt-4o-mini\"\n",
    "    run_llm(prompt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "%%time\n",
    "for i, spm in enumerate(spm_liste):\n",
    "    print(f'Spm {i}: {spm}')\n",
    "    prompt = f\"{spm}. Svar på norsk. Bruk max 150 ord. Start med 'Svar på Spm {i}'. Ikke bruk avsnitt. -m l31i\"\n",
    "    run_llm(prompt)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "%%time\n",
    "for i, spm in enumerate(spm_liste):\n",
    "    print(f'Spm {i}: {spm}')\n",
    "    prompt = f\"{spm}. Svar på norsk. Bruk max 150 ord. Start med 'Svar på Spm {i}'. Ikke bruk avsnitt. -m claude-3.5-sonnet\"\n",
    "    run_llm(prompt)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "%%time\n",
    "poet = \"Herman Wildenvvey\"\n",
    "prompt = f\"Write a poem in the style of {poet} in Norwegian -m deepseek-r1:70b\"\n",
    "run_llm(prompt)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\>>> Write a poem in the style of Herman Wildenvvey in Norwegian\n",
    "\n",
    "<think>\n",
    "Okay, so I need to write a poem in the style of Herman Wildenvey. Hmm, I'm \n",
    "not super familiar with him, but I think he's a Norwegian poet known for \n",
    "his lyrical and nature-inspired works. Let me see... I remember his poems \n",
    "often have a gentle tone and focus on the beauty of everyday life, maybe \n",
    "with some romantic elements.\n",
    "\n",
    "\\<think>\n",
    "\n",
    "Okay, so I need to write a poem in the style of Herman Wildenvey. Hmm, I'm \n",
    "not super familiar with him, but I think he's a Norwegian poet known for \n",
    "his lyrical and nature-inspired works. Let me see... I remember his poems \n",
    "often have a gentle tone and focus on the beauty of everyday life, maybe \n",
    "with some romantic elements.\n",
    "\n",
    "\n",
    "Alright, so first, I should probably figure out what makes Wildenvey's \n",
    "style unique. From what I recall, he uses simple yet evocative language, \n",
    "often depicting natural scenes and emotions tied to them. His poems can be \n",
    "introspective, sometimes melancholic but also hopeful.\n",
    "\n",
    "I need to write this in Norwegian, which is a bit tricky since I'm more \n",
    "comfortable with English. Maybe I should start by brainstorming some \n",
    "themes or images that are typical for Wildenvey—like forests, seasons \n",
    "changing, maybe love or solitude. \n",
    "\n",
    "Let me think about the structure. Wildenvey's poems often have a rhyme \n",
    "scheme, perhaps ABAB or something similar. The lines might be of varying \n",
    "lengths but with a rhythmic flow.\n",
    "\n",
    "I should also consider the tone. It should be lyrical and contemplative, \n",
    "capturing moments in nature that evoke deeper feelings. Maybe starting \n",
    "with an image like twilight or morning, which are common in his work.\n",
    "\n",
    "Okay, I'll start drafting some lines. Let me think about the seasons—maybe \n",
    "autumn since it's a time of change and reflection. Leaves falling, maybe a \n",
    "quiet forest scene. Using words that paint vivid pictures without being \n",
    "too complex.\n",
    "\n",
    "I need to make sure the Norwegian is correct, so perhaps using simple \n",
    "vocabulary but in a poetic way. Maybe include some alliteration or \n",
    "assonance to give it that lyrical feel.\n",
    "\n",
    "Wait, I should also think about meter. Wildenvey's poems often have a \n",
    "consistent meter, which adds to their musicality. I'm not great at \n",
    "counting syllables, but maybe I can get a rough iambic tetrameter going.\n",
    "\n",
    "Let me try putting this together. Start with a scene in the forest during \n",
    "autumn. Use some sensory details—sight, sound, smell. Maybe personify \n",
    "elements of nature to give it that poetic touch.\n",
    "\n",
    "I should also include some emotional undertone, perhaps a sense of longing \n",
    "or peace. Maybe something about memories or the passage of time.\n",
    "\n",
    "Hmm, okay, so putting this into verse. I'll try to write a stanza and see \n",
    "how it flows. Then maybe expand from there, ensuring each stanza connects \n",
    "with the next thematically.\n",
    "\n",
    "I also need to make sure the poem isn't too long—maybe four stanzas? That \n",
    "seems manageable and allows for a complete thought without being \n",
    "overwhelming.\n",
    "\n",
    "Alright, let's start writing. I'll take my time with each line, ensuring \n",
    "it captures the essence of Wildenvey while maintaining proper Norwegian \n",
    "grammar and poetic elements.\n",
    "\n",
    "\\</think>\n",
    "\n",
    "In twilight's hush, where shadows softly fall,  \n",
    "The forest whispers secrets to us all.  \n",
    "Autumn's palette paints the trees in gold,  \n",
    "A fleeting dance before the winter's cold.  \n",
    "\n",
    "Beneath the leaves, a carpet soft and deep,  \n",
    "Echoes of memories in silent sleep.  \n",
    "Each rustling sound a story unfolds,  \n",
    "Of moments past where love once dwelled.  \n",
    "\n",
    "The wind carries scents both earthy and sweet,  \n",
    "As nature's final waltz begins to beat.  \n",
    "In this quiet hour, I find my peace,  \n",
    "Where solitude and calmness never cease.  \n",
    "\n",
    "So let the seasons turn their gentle page,  \n",
    "For in each change, a new tale's stage.  \n",
    "And though the leaves may fade from sight,  \n",
    "Their memory stays, a guiding light.\n",
    "\n",
    "\\>>> Send a message (/? for help)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmed365-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
